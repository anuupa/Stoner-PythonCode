\documentclass[a4paper,11pt]{scrartcl}
\usepackage[dvips]{graphicx}
\usepackage[twoside,paper=a4paper,hmarginratio=3:2,tmargin=2.5cm,bmargin=3cm]{geometry}
\usepackage{scrpage2}
\usepackage{amsmath,amsbsy,amsfonts,amssymb,amsxtra}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{gb_custom}

\setlength\marginparsep{0cm}

\graphicspath{{./figures/}}


\reversemarginpar


\author{C.S.~Allen, M.~Newman and G.~Burnell}
\title{Stoner Python Package}

\begin{document}

\maketitle

\tableofcontents
\newpage
\pagestyle{scrheadings} \ihead[Stoner Python Package]{Stoner Python Package} \ifoot[\today]{\today}
\ohead[Manual]{Manual}



  \section{Introduction}

This manual provides a user guide and reference for the Stoner python pacakage. The Stoner python package provides a set of python classes and functions for reading, manipulating and plotting data acquired with the lab equipment in the Condensed Matter Physics Group at the University of Leeds.

\subsection{Getting the Stoner Package}

The source code for the Stoner python module is kept in CVS revision control on the stonerlab server. A stable release of the code is available for copying and use in \verb#\\stonerlab\data\software\python\stable\#. The development code can be obtained by checking out the PythonCode module with a CVSROOT of \\ \verb#:ext:cvs@stonerlab.leeds.ac.uk:/home/cvs/#. Appropriate ssh keys for the cvs user account are kept in \verb#\\stonerlab\data\software\CVS\#.

The Stoner Package currently depends on a number of other modules. These are installed on the lab machines that have Python installed. Primarily these are Numpy, SciPy and Matplotlib. Windows installable versions are kept in \\ \verb#\\stonerlab\\data\software\Python\#.  The easiest way to get a Python installation with all the necessary dependencies for the Stoner Package is to install the \textit{Enthought Python Distribution}. Windows install file are kept in \verb#\\stonerlab\data\software\python#


\subsection{Using the Stoner Package}

The easiest way to use the Stoner Package is to add the path to the directory containing Stoner.py to your PYTHONPATH environment variable. This can be done on Macs and Linux by doing:
\begin{verbatim}
  cd <path to PythonCode directory>/src
  export PYTHONPATH=`pwd`:$PYTHONPATH
\end{verbatim}
On a windows machine the easiest way is to create a permanent entry to the folder in the system environment variables. Go to Control Panel -> System -> Advanced Tab -> click on Environment button and then add or edit an entry to the system variable PYTHONPATH.

One this has been done, the Stoner module may be loaded from python command line:

\begin{verbatim}
  >>> import Stoner
\end{verbatim}

or

\begin{verbatim}
  >>> from Stoner import *
\end{verbatim}

\section{Users' Guide}

The Users'Guide provides a brief overview of the functions contained within the Stoner module and so basic examples of how the module can be used.

The Stoner module provides several Python classes that can be used to manipulate experimental data. The main class that provides the basic functionality is the DataFile class. This handles loading data, finding and manipulating meta data, selecting rows or columns of data, adding or removing data, and saving data.

The PlotFile class is a descendent of DataFile, meaning it shares all the same functionality as DataFile, but in addition has methods to present data graphically. The AnalyseFile class is another descendent of DataFile, but provides extra methods to fit curves, smooth and differentiate data, find peaks and carry out other simple analysis operations.

\subsection{Loading a data file}

The first step in using the Stoner module is to load some data from a measurement.

\begin{verbatim}
  >>>import Stoner
  >>>d=Stoner.DataFile('my_data.txt')
  >>>d=Stoner.DataFile('my_VSM_data.fld','VSM')
\end{verbatim}

In this example we have loaded data from my\_data.txt which should be in the current directory -- here we are assuming that my\_data.txt contains data in the \textit{TDI Format 1.5} which is produced by the LabVIEW rigs. Assuming that the file successfully loads, \textit{d}, is an instance of the DataFile object. Here the DataFile constructor has been used to both create the instance and load the data in one go. The second version adds an extra parameter that specifying how to interpret the file data. Currently allowed values are \textit{TDI}, \textit{VSM}, \textit{BigBlue}, and \textit{csv}. It is also possible to do these steps separately, or indeed to load new data into an existing instance of DataFile.

\begin{verbatim}
  >>>import Stoner
  >>>d=Stoner.DataFile()
  >>>d.load('my_data.txt')
  >>>d.load('my_VSM_data.fld','VSM')
\end{verbatim}

The \textit{BigBlue} version of the DataFile.load and DataFile constructors takes two additional parameters that specify the row on which the column headers will be found and the row on which the data starts. The \textit{csv} takes four additional parameters to the constructor and load methods. In addition to the two extra arguments used for the \textit{BigBlue} variant, a further two parameters specify the deliminators for the header and data rows.

\keypoint{The load method, like many of the DataFile methods returns a copy of the Datafile object \textbf{as well as} modifying the object itself. The advantage of this is that it is then possible to chain several methods into one command}

\subsection{Examining Some Data}
\subsubsection{Data, Column headers and metadata}
Having loaded some data, the next stage might be to take a look at it. Internally, data is represented as a 2D numpy array of floating point numbers, along with a list of column headers and two dictionaries that hold metadata and type information about metadata (metametadata perhaps !). These can be accessed like so:
\begin{verbatim}
  >>>d.data
  >>>d.column_headers
  >>>d.metadata
  >>>d.typehint
\end{verbatim}

\subsubsection{Working with columns of data}

This is all very well, but often you want to examine a particular column of data or a particular row:
\begin{verbatim}
  >>>d.column(0)
  >>>d.column('Temperature')
  >>>d.column(['Temperature',0])
\end{verbatim}
In the first example, the first column of numeric data will be returned. In the second example, the column headers will first be checked for one labeled exactly \textit{Temperature} and then if no column is found, the column headers will be searched using \textit{Temperature} as a regular expression. This would then match \textit{Temperature (K)} or \textit{Sample Temperature}.  The third example results in a 2 dimensional numpy array containing two columns in the order that they appear in the list (\ie not the order that they are in the data file). For completeness, the \textbf{DataFile.column} method also allows one to pass slices to select columns and should do the expected thing.

\subsubsection{Working with complete rows of data}

Rows don't have labels, so are accessed directly by number:
\begin{verbatim}
  >>>d[1]
  >>>d[1:4]
\end{verbatim}
The second example uses a slice to pull out more than one row. This syntax also supports the full slice syntax which allows one to, for example, decimate the rows, or directly pull out the last fews rows in the file.

\subsubsection{Manipulating the metadata}

What happens if you use a string and not a number in the above examples ?
\begin{verbatim}
  >>>d['User']
\end{verbatim}
in this case, it is assumed that you meant the metadata with key \textit{User}. To get a list of possible keys in the metadata, you can do:
\begin{verbatim}
  >>>d.dir()
  >>>d.dir('Option\:.*')
\end{verbatim}
In the first case, all of the keys will be returned in a list. In the second, only keys matching the pattern will be returned -- all keys containing \textit{Option:}.

\subsubsection{Selecting Individual rows and columns of data}

Sometimes you may want to iterate over all of the rows or columns in a data set. This can be done quite easily:
\begin{verbatim}
  >>>for row in d.rows():
  ......print row
  ......
  >>>for column in d.columns():
  ......print column
  ......
\end{verbatim}
The first example could also have been written more compactly as:
\begin{verbatim}
  >>>for row in d:
  ......print row
  ......
\end{verbatim}

In many cases you do not know which rows in the data file are of interest - in this case you want to search the data.
\begin{verbatim}
  >>>d.search('Temperature',4.2)
  >>>d.search('Temperature',4.2,['Temperature','Resistance'])
  >>>d.search('Temperature',lambda x,y: x>10 and x<100)
  >>>d.search('Temperature',lambda x,y: x>10 and
                x<1000 and y[1]<1000,['Temperature','Resistance'])
\end{verbatim}
The general form is \\\verb:DataFile.search(<search column>,<search term>[,<list of return columns>]):

The first example will return all the rows where the value of the \textit{Tenperature} column is 4.2. The second example is the same, but only returns the values from the \textit{Temperature}, and \textit{Resistance} columns. The rules for selecting the columns are the same as for the DataFile.column method above -- strings are matched against column headers and integers select column by number.

The third and fourth examples above demonstrate the use of a function as the search value. This allows quite complex search criteria to be used. The function passed to the search routine should take two parameters -- a floating point number and a numpy array of floating point numbers and should return either \textit{ture} or \textit{False}. The function is evaluated for each row in the data file and is passed the value corresponding to the search column as the first parameter while the second parameter contains all of the values in the row to be returned. If the search function returns True, then the row is returned, otherwise it isn't.

\subsubsection{Find out more about the data}

Another question you might want to ask is, what are all the unique
values of data in a given column (or set of columns). The Python numpy
package has a function to do this and we have a direct pass through
from the DataFile object for this:

\begin{verbatim}
  >>> d.unique('Temp')
  >>> d.unique(column,return_index=False, return_inverse=False)
\end{verbatim}

The two optional keywords cause the numpy routine to return the
indices of the unique and all non-unique values in the array. The
column is specified in the same way as the \textbf{DataFile.column}
method does.


\subsection{Modifying Data}

\subsubsection{Appending data}

The simplest way to modify some data might be to append some columns or rows. The Stoner mpodule redefines two standard operators, \verb:+: and \verb:&: to have special meanings:
\begin{verbatim}
  >>>a=Stoner.DataFile('some_new_data.txt')
  >>>add_rows=d+a
  >>>add_columns=d&a
\end{verbatim}
In these example, \textit{a} is a second DataFile object that contains some data. In the first example, a new DataFile object is created where the contents of \textit{a} are added as new rows after the data in \textit{d}. Any metadata that is in \textit{a} and not in \textit{d} are added to the metadata as well. There is a requirement, however, that the column headers of \textit{d} and \textit{a} are the same -- \ie that the two DataFile objects appear to represent similar data.

In the second example, the data in \textit{a} is added as new columns after the data from \textit{d}. In this case, there is a requirement that the two DataFile objects have the same number of rows.

These operators are not limited just to DataFile objects, you can also add numpy arrays to the DataFile object to append additional data.
\begin{verbatim}
  >>>import numpy as np
  >>>x=np.array([1,2,3])
  >>>new_data=d+x
  >>>y=np.array([1,2,3],[11,12,13],[21,22,23],[31,32,33]])
  >>>new_data=d+y
  >>>column=d.column[0]
  >>>new_data=d&column
\end{verbatim}
In the first example above, we add a single row of data to \textit{d}. This assumes that the number of elements in the array matches the number of columns in the data file. The second example is similar but this time appends a 2 dimensional numpy array to the data. The third example appends a numpy array as a column to \textit{d}. In this case the requirement is that the numpy array has the same or fewer rows of data as \textit{d}.

\subsubsection{Inserting Columns of Data}

The append columns operator \verb#&# will only add columns to the end of a dataset. If you want to add a column of data in the middle of the data set then you should use the \textbf{add\_column} method.

\begin{verbatim}
  >>>d.add_column(numpy.array(range(100)),'Column Header')
  >>>d.add_column(numpy.array(range(100)),'Column Header',Index)
  >>>d.add_column(lambda x: x[0]-x[1],'Column Header',func_args=None)
\end{verbatim}

The first example simply adds a column of data to the end of the dataset and sets the new column headers. The second variant  inserts the new column before column \textit{Index}. \textit{Index} follows the same rules as for the \textbf{DataFile.colummn()} method. In the third example, the new column data is generated by applying the specified function. The function is passed s dingle row as a 1D numpy array and any of the keyword, argument pairs passed in a dictionary to the optional \textit{func\_args} argument.

The \textbf{DataFile.add\_column} method returns a copy of the DataFile object itself as well as modifying the object. This is to allow the metod to be chained up with other methods for more compact code writing.

\subsubsection{Deleting Rows of Data}

Removing complete rows of data is achieved using the \textbf{DataFile.del\_row} method.

\begin{verbatim}
  >>>d.del_rows(10)
  >>>d.del_rows('X Col',value)
  >>>d.del_rows('X Col',lambda x,y:x>300)
\end{verbatim}

The first variant will delete row 10 from the data set (where the first row will be row 0). You can also supply a list or slice to \textbf{DataFile.del\_rows} to delete multiple rows.

If you do not know in advance which row to delete, then the second and third variants provide more advanced options. The second variant searches for and deletes all rows in which the specified column contains \textit{value}. The third variant selects which ros to delete by calling a user supplied function for each row. The user supplied function is the same in form and definitition as that used for the \textbf{DataFile.search} method.

\subsubsection{Deleting Columns of Data}

Deleting whole columns of data can be done by referring to a column by index or column header - the indexing rules are the same as used for the \textbf{DataFile.column} method.

\begin{verbatim}
  >>>d.del_column('Temperature')
  >>>d.del_column(1)
\end{verbatim}

\subsection{Saving Data}

Only saving data in the \textit{TDI} format is supported.

\begin{verbatim}
  >>>d.save()
  >>>d.save(filename)
\end{verbatim}

In the first case, the filename used tosave the data is determined from the filename attribute of the DataFile object. This will have been set when the filewas loaded from disc.

If the filename attribute has not been set \eg if the DataFile object was created from scratch, then the \textbf{DataFile.save} method will cause a dialog box to be raised so that the user can supply a filename.

In the second variant, the supplied filename is used and the filename attribute is changed to match this \ie \verb#d.filename# will always return the last filename used for a load or save operation.

\subsection{Plotting Data}

\subsection{Curve Fitting}

Curve fitting is handled by a sub-class of the DataFile object -- AnalyseFile

\begin{verbatim}
  >>> import Stoner.Analysis as Analysis
  >>> a=Analysis.AnalyseFile('Data')
  >>>  a2=Analysis.AnalyseFile()
  >>> a2=d
\end{verbatim}

The first line imports the AnaylseFile class. Since the AnalyseFile is a child class of DataFile, everything you can do with a DataFile also works with an AnalyseFile object. The last two lines demonstrate creating a blank AnalyseFile and then copying all of the data, metadata and column headings from an existing dataFile object.

\subsubsection{Simply polynomial Fits}
\begin{verbatim}
  >>> a.polyfit(column_x,column_y,polynomial_order, bounds=lambda x, y:True)
\end{verbatim}

\subsubsection{Simple function fitting}
\begin{verbatim}
  >>> a.curve_fit(func,  xcol, ycol, p0=None, sigma=None, bounds=lambda x, y: True )
\end{verbatim}

\subsubsection{Fitting with limits}
\begin{verbatim}
  >>> a.mpfit(func,  xcol, ycol, p_info,  func_args=dict(), sigma=None, 
               bounds=lambda x, y: True, **mpfit_kargs )
\end{verbatim}

\subsection{More AnalyseFile Functions}

\subsubsection{Basic Data Inspection}
\begin{verbatim}
  >>> a.max(column)
  >>> a.min(column)
\end{verbatim}

\subsubsection{Thresholding and Interpolating Data}
\begin{verbatim}
  >>> a.threshold(col, threshold, rising=True, falling=False)
  >>> a.interpolate(newX,kind='linear' )
\end{verbatim}

\section{Cookbook}

This section gives some short examples to give an idea of things that can be done with the Stoner python module in just a few lines.

\subsection{Extract X-Y(Z) from X-Y-Z data}

In a number of measurement systems the data is returned as 3 parameters X, Y and Z and one wishes to extract X-Y as a function of constant Z. For example, $I-V$ sweeps as a function of gate voltage $V_G$. Assuming we have a data file with columns \textit{Current}, \textit{Voltage},\textit{Gate}:

\begin{verbatim}
  >>> d=DataFile('data.txt')
  >>> t=d
  >>> for gate in d.unique('Gate'):
  >>>     t.data=d.search('Gate',gate)
  >>>     t.save('Data Gate='+str(gate)+'.txt')
\end{verbatim}

The first line opens the data file containing the $I-V(V_G)$ data. The second creates a temporary copy of the DataFile object - ensuring that we get a copy of all metadata and column headers. The \textbf{for} loop iterates over all unique values of the data in the gate column and then inside the for loop, searches for the corresponding $I-V$ data, sets it as the data of the temporary DataFile and then saves it.
\end{document}
